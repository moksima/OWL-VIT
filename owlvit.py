# -*- coding: utf-8 -*-
"""OWLVIT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-F8BYDD3IebjVih0lqc1D2TJnVADu2H
"""

# CELL 1 — install dependencies
!pip install git+https://github.com/facebookresearch/segment-anything.git > /dev/null
!pip install transformers[torch] ftfy regex tqdm matplotlib pillow torchvision > /dev/null

# CELL 2 — imports & model setup
import os
import torch
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from transformers import (
    OwlViTProcessor, OwlViTForObjectDetection,
    CLIPProcessor, CLIPModel
)
from segment_anything import (
    sam_model_registry, SamPredictor, SamAutomaticMaskGenerator
)
from torchvision.ops import nms

# device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1) OWLViT
processor = OwlViTProcessor.from_pretrained("google/owlvit-large-patch14")
owlvit    = OwlViTForObjectDetection.from_pretrained("google/owlvit-large-patch14").to(device)

# 2) SAM (+ predictor + automatic mask gen)
sam_ckpt = "sam_vit_h.pth"
if not os.path.exists(sam_ckpt):
    !wget -q -O {sam_ckpt} https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
sam       = sam_model_registry["vit_h"](checkpoint=sam_ckpt).to(device)
predictor = SamPredictor(sam)
mask_generator = SamAutomaticMaskGenerator(
    model=sam,
    points_per_batch=64,
    stability_score_thresh=0.7,
    box_nms_thresh=0.4,
    min_mask_region_area=1500
)

# 3) CLIP zero-shot
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
clip_model     = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)

print("✅ Models loaded")

# CELL 3 — upload images from local machine and set up directories
from google.colab import files
import os

# where to store input images and the output masks
spirent_image_dir = '/content/SpirentUnlabeled'
pseudo_mask_dir   = '/content/predicted_masks'
os.makedirs(spirent_image_dir, exist_ok=True)
os.makedirs(pseudo_mask_dir,   exist_ok=True)

# prompt to upload
uploaded = files.upload()
for fname, data in uploaded.items():
    with open(os.path.join(spirent_image_dir, fname), 'wb') as f:
        f.write(data)

print(f"✅ Uploaded {len(uploaded)} files to {spirent_image_dir}")

# CELL 3b — define material+tree classes


MATERIALS = [
    "concrete",
    "door",
    "glass",
    "wood",
    "soil",
    "brick",
    "plastic",
    "tile",
    "metal",
    "tree"
]

# Synonyms mapping, if used later
SOIL_EQUIV = {
    "dirt":  "soil",
    "earth": "soil",
    "ground":"soil"
}

print("✅ MATERIALS defined:", MATERIALS)

# CELL 4 — run_owlvit_sam_tree_separate with confidence logging
import os
import torch
import numpy as np
import cv2
from PIL import Image
from torchvision.ops import nms

def run_owlvit_sam_tree_separate(
    image_path,
    prompts,                  # include "tree" & other classes
    min_score=0.05,
    nms_iou=0.4,
    tile_size=512,
    overlap=128,
    scales=(0.75,1.0,1.25),
    top_k=10,
    min_area_ratio=0.001,
    clip_thresh=0.3,
    morph_kernel=15,
    leaf_overlap_thresh=0.10  # if >10% of auto‐mask overlaps tree, mark as tree
):
    # load & prep
    img = Image.open(image_path).convert("RGB")
    W, H = img.size
    img_np = np.array(img)
    predictor.set_image(img_np)

    final = np.zeros((H, W), dtype=np.uint8)
    used  = np.zeros((H, W), dtype=bool)
    confidences = []  # collect dicts of confidences

    # STEP 0: tree detection
    tree_idx = prompts.index("tree")
    inp = processor(text=["tree"], images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        out = owlvit(**inp)
    tgt = torch.tensor([[H, W]]).to(device)
    det = processor.post_process_object_detection(out, target_sizes=tgt, threshold=min_score)[0]
    for box, score in zip(det["boxes"], det["scores"]):
        if score < min_score: continue
        x0,y0,x1,y1 = map(int, box.cpu().numpy())
        masks, iou_preds, _ = predictor.predict(
            box=np.array([x0,y0,x1,y1])[None],
            multimask_output=True
        )
        best_i = int(iou_preds.argmax())
        mask_iou = float(iou_preds[best_i])
        best_mask = masks[best_i].astype(bool)

        final[best_mask] = tree_idx + 1
        used |= best_mask

        confidences.append({
            "phase": "tree-detect",
            "label": "tree",
            "box_score": float(score.cpu()),
            "mask_iou": mask_iou,
            "stability_score": None,
            "clip_score": None
        })

    # STEP 1: propose boxes for non-tree
    all_boxes, all_labels, all_scores = [], [], []
    for scale in scales:
        ws, hs = int(W*scale), int(H*scale)
        scaled = img.resize((ws, hs), Image.BILINEAR)
        for x0 in range(0, ws, tile_size - overlap):
            for y0 in range(0, hs, tile_size - overlap):
                x1, y1 = min(x0+tile_size, ws), min(y0+tile_size, hs)
                crop = scaled.crop((x0,y0,x1,y1))
                inp = processor(text=prompts, images=crop, return_tensors="pt").to(device)
                with torch.no_grad():
                    out = owlvit(**inp)
                tgt_crop = torch.tensor([[crop.height, crop.width]]).to(device)
                det = processor.post_process_object_detection(
                    out, target_sizes=tgt_crop, threshold=min_score
                )[0]
                for box, lab, sc in zip(det["boxes"], det["labels"], det["scores"]):
                    if sc < min_score or prompts[int(lab)]=="tree":
                        continue
                    bx0,by0,bx1,by1 = box.cpu().numpy()
                    X0 = max(0, int((bx0 + x0)/scale))
                    Y0 = max(0, int((by0 + y0)/scale))
                    X1 = min(W, int((bx1 + x0)/scale))
                    Y1 = min(H, int((by1 + y0)/scale))
                    if (X1-X0)*(Y1-Y0) < min_area_ratio*W*H:
                        continue
                    all_boxes.append([X0,Y0,X1,Y1])
                    all_labels.append(int(lab.cpu()))
                    all_scores.append(float(sc.cpu()))

    # STEP 2: NMS + top_k
    proposals = []
    if all_boxes:
        boxes_t  = torch.tensor(all_boxes, dtype=torch.float32, device=device)
        scores_t = torch.tensor(all_scores, dtype=torch.float32, device=device)
        keep     = nms(boxes_t, scores_t, nms_iou).cpu().tolist()[:top_k]
        for i in keep:
            proposals.append((all_boxes[i], all_labels[i], all_scores[i]))

    # STEP 3: SAM per proposal
    for (x0,y0,x1,y1), lab, sc in proposals:
        masks, iou_preds, _ = predictor.predict(
            box=np.array([x0,y0,x1,y1])[None],
            multimask_output=True
        )
        best_i = int(iou_preds.argmax())
        mask_iou = float(iou_preds[best_i])
        best_mask = masks[best_i].astype(bool)
        newpix = best_mask & (~used)

        final[newpix] = lab + 1
        used |= newpix

        confidences.append({
            "phase": "proposal-mask",
            "label": prompts[lab],
            "box_score": sc,
            "mask_iou": mask_iou,
            "stability_score": None,
            "clip_score": None
        })

    # STEP 4: auto masks + CLIP
    auto_ms = mask_generator.generate(img_np)
    for m in auto_ms:
        seg = m["segmentation"] & (~used)
        area = seg.sum()
        if area < min_area_ratio*W*H:
            continue

        pred_iou = float(m["predicted_iou"])
        stab     = float(m["stability_score"])

        # record the raw auto-mask confidence
        confidences.append({
            "phase": "auto-mask",
            "label": "auto",
            "box_score": None,
            "mask_iou": pred_iou,
            "stability_score": stab,
            "clip_score": None
        })

        # leaf-overlap → assign tree
        if (seg & (final==(tree_idx+1))).sum() / area > leaf_overlap_thresh:
            final[seg] = tree_idx + 1
            used |= seg
            continue

        # otherwise CLIP classify
        ys, xs = np.where(seg)
        y0n,y1n = ys.min(), ys.max()+1
        x0n,x1n = xs.min(), xs.max()+1
        roi = img.crop((x0n,y0n,x1n,y1n))
        rnp = np.array(roi)
        sub = seg[y0n:y1n, x0n:x1n]
        rnp[~sub] = 0

        clip_in = clip_processor(
            text=prompts,
            images=Image.fromarray(rnp),
            return_tensors="pt",
            padding=True
        ).to(device)
        with torch.no_grad():
            logits = clip_model(**clip_in).logits_per_image.softmax(-1)[0]
        p, idx = float(logits.max().item()), int(logits.argmax().item())
        if p < clip_thresh:
            continue

        final[seg] = idx + 1
        used |= seg

        confidences.append({
            "phase": "auto-clip",
            "label": prompts[idx],
            "box_score": None,
            "mask_iou": None,
            "stability_score": None,
            "clip_score": p
        })

    # STEP 5: morphological cleanup
    clean = np.zeros_like(final)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (morph_kernel,)*2)
    for i in range(1, len(prompts)+1):
        bm = (final==i).astype(np.uint8)
        n,labs = cv2.connectedComponents(bm)
        comp = np.zeros_like(bm)
        for j in range(1,n):
            part = (labs==j).astype(np.uint8)
            if part.sum() >= min_area_ratio*W*H:
                comp |= part
        comp = cv2.morphologyEx(comp, cv2.MORPH_CLOSE, kernel)
        comp = cv2.morphologyEx(comp, cv2.MORPH_OPEN,  kernel)
        clean[comp>0] = i

    return clean, confidences

# ─── CELL 4 — define  material classes (and soil synonyms) ─────────────────

# the labels
MATERIALS = [
    "concrete","door","glass","wood","soil",
    "brick","plastic","tile","metal"
]

# map any synonyms back to "soil"
SOIL_EQUIV = {
    "dirt": "soil",
    "earth":"soil",
    "ground":"soil"
}

print("✅ MATERIALS and SOIL_EQUIV defined:", MATERIALS)

# CELL 5 — batch‐run, collect all confidences and compute averages
from tqdm import tqdm
from PIL import Image
import os
import pandas as pd

# 1) ensure “tree” is in class list
classes = MATERIALS.copy()
if "tree" not in classes:
    classes.append("tree")

all_confs = []

# 2) loop over every image
for fn in tqdm(os.listdir(spirent_image_dir)):

    # skip non‐images
    if not fn.lower().endswith(('.jpg','jpeg','png')):
        continue

    path = os.path.join(spirent_image_dir, fn)
    mask, confidences = run_owlvit_sam_tree_separate(path, prompts=classes)

    # save mask as before
    stem = os.path.splitext(fn)[0]
    Image.fromarray(mask).save(
        os.path.join(pseudo_mask_dir, f"{stem}_mask.png")
    )

    # collect all the confidences
    for c in confidences:
        # add the image name
        c['image'] = fn
        all_confs.append(c)

# 3) build a DataFrame
df = pd.DataFrame(all_confs)

# 4) compute mean ± std for each phase
summary = (
    df
    .groupby('phase')
    .agg({
        'box_score': ['mean','std'],
        'mask_iou':  ['mean','std'],
        'stability_score': ['mean','std'],
        'clip_score': ['mean','std']
    })
)

print(summary)

# CELL 6 — visualize each sample in its own figure
import random
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import os

# Ensure the class list includes "tree"
classes = MATERIALS.copy()
if "tree" not in classes:
    classes.append("tree")

# pick up to 4 random masks
mask_files = [f for f in os.listdir(pseudo_mask_dir) if f.endswith("_mask.png")]
samples    = random.sample(mask_files, min(6, len(mask_files)))

# shared colormap
cmap = plt.get_cmap("tab20", len(classes) + 1)

# directory to save visualizations
out_dir = os.path.join(pseudo_mask_dir, "viz")
os.makedirs(out_dir, exist_ok=True)

for mname in samples:
    stem     = mname.replace("_mask.png", "")
    img_path = next(Path(spirent_image_dir).glob(f"{stem}.*"))
    orig     = np.array(Image.open(img_path).convert("RGB"))
    mask     = np.array(Image.open(os.path.join(pseudo_mask_dir, mname)))

    # get list of labels present
    uids = np.unique(mask)
    lbls = [classes[uid-1] for uid in uids if 0 < uid <= len(classes)]

    # make a new figure for this sample
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # 1) Original
    axes[0].imshow(orig)
    axes[0].set_title("Original")
    axes[0].axis("off")

    # 2) Raw Mask
    axes[1].imshow(mask, cmap=cmap, vmin=0, vmax=len(classes))
    axes[1].set_title("Raw Mask")
    axes[1].axis("off")

    # 3) Overlay
    axes[2].imshow(orig)
    axes[2].imshow(mask, cmap=cmap, vmin=0, vmax=len(classes), alpha=0.5)
    axes[2].set_title("Overlay: " + (", ".join(lbls) if lbls else "none"))
    axes[2].axis("off")

    plt.tight_layout()
    plt.show()

    # optionally save to disk:
    fig.savefig(os.path.join(out_dir, f"{stem}_viz.png"), dpi=150)
    plt.close(fig)

# CELL 7 — overlay + per-image legend (with “tree” included)

import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from PIL import Image
import matplotlib.patches as mpatches

# ─── Adjust these to match Colab paths ───────────────────────────────
spirent_image_dir = '/content/SpirentUnlabeled'
pseudo_mask_dir   = '/content/predicted_masks'
# ─────────────────────────────────────────────────────────────────────────

# Use MATERIALS list
classes = MATERIALS.copy()
if "tree" not in classes:
    classes.append("tree")

# Build the same colormap used for masks
cmap = plt.get_cmap("tab20", len(classes) + 1)

# Find all the images uploaded
image_files = sorted([
    f for f in os.listdir(spirent_image_dir)
    if f.lower().endswith(('.jpg','jpeg','png'))
])

# Create a subplot for each image: [overlay | legend]
fig, axes = plt.subplots(len(image_files), 2, figsize=(10, 5 * len(image_files)))
if len(image_files) == 1:
    axes = np.expand_dims(axes, 0)

for i, fname in enumerate(image_files):
    img_path  = os.path.join(spirent_image_dir, fname)
    mask_path = os.path.join(pseudo_mask_dir, f"{os.path.splitext(fname)[0]}_mask.png")

    # Load originals & masks
    orig = np.array(Image.open(img_path).convert("RGB"))
    mask = np.array(Image.open(mask_path))

    # ─── Overlay plot ─────────────────────────────────────────────────────────
    ax_img = axes[i, 0]
    ax_img.imshow(orig)
    ax_img.imshow(mask, cmap=cmap, vmin=0, vmax=len(classes), alpha=0.5)
    ax_img.set_title(f"{fname} (Overlay)")
    ax_img.axis("off")

    # ─── Legend plot ─────────────────────────────────────────────────────────
    ax_leg = axes[i, 1]
    unique_ids = np.unique(mask)
    patches = []
    for uid in unique_ids:
        if uid == 0:
            continue
        color = cmap(uid)
        label = classes[uid - 1]
        patches.append(mpatches.Patch(color=color, label=label))
    ax_leg.legend(handles=patches, loc='center', title="Detected Materials")
    ax_leg.axis("off")

plt.tight_layout()
plt.show()



# CELL 7 — visualize OWL-ViT zero-shot boxes on all uploaded images
import matplotlib.pyplot as plt
from PIL import Image
import torch
import os

def show_owlvit_boxes(image_path, prompts, threshold=0.25):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(text=prompts, images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = owlvit(**inputs)
    target_sizes = torch.tensor([[image.height, image.width]]).to(device)
    results = processor.post_process_object_detection(
        outputs, target_sizes=target_sizes, threshold=threshold
    )[0]

    plt.figure(figsize=(6,6))
    plt.imshow(image)
    ax = plt.gca()
    for box, label, score in zip(results["boxes"], results["labels"], results["scores"]):
        x1, y1, x2, y2 = box.tolist()
        ax.add_patch(plt.Rectangle(
            (x1, y1), x2-x1, y2-y1,
            edgecolor="red", fill=False, linewidth=2
        ))
        ax.text(
            x1, y1 - 5,
            f"{prompts[label]} {score:.2f}",
            color="white", fontsize=10,
            bbox=dict(facecolor='red', alpha=0.5, pad=2)
        )
    plt.axis("off")
    plt.show()

# Loop through every image uploaded
for fname in sorted(os.listdir(spirent_image_dir)):
    if not fname.lower().endswith(('.jpg','jpeg','png')):
        continue
    print(f"Image: {fname}")
    show_owlvit_boxes(
        os.path.join(spirent_image_dir, fname),
        MATERIALS,      # or PROMPTS, whichever list used
        threshold=0.25  # adjust confidence threshold as needed
    )

# ── Cell X: Plot per‐phase confidence distributions ─────────────────────────

import matplotlib.pyplot as plt


phases = df['phase'].unique()
metrics = ['box_score', 'mask_iou', 'stability_score', 'clip_score']

for metric in metrics:
    plt.figure(figsize=(6,4))
    for phase in phases:
        vals = df.loc[df['phase']==phase, metric].dropna()
        if not vals.empty:
            plt.hist(vals, bins=20, alpha=0.6, label=phase)
    plt.title(f'Distribution of {metric}')
    plt.xlabel(metric)
    plt.ylabel('Count')
    plt.legend()
    plt.tight_layout()
    plt.show()

