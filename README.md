# OWL-ViT – Open-Vocabulary Object Detection (Colab Demo)

This repository contains a Google Colab notebook demonstrating **open-vocabulary
object detection** using **OWL-ViT** (Open-World Localization Vision Transformer).

The notebook was developed as part of MSc-level coursework and research to explore
vision–language models for object detection beyond fixed, closed-set label spaces.

## Contents
- Vision–language object detection with OWL-ViT
- Text-prompt-based object queries
- Image preprocessing and inference
- Visualization of detected objects and bounding boxes
- Experiments with flexible, open-set categories

## Purpose
This repository is provided as a **demonstration of practical experience** with
open-vocabulary and open-set computer vision models, focusing on the interaction
between textual prompts and visual representations.

The work is exploratory and intended to illustrate familiarity with modern
vision–language architectures rather than to present a finalized research system.

## How to run
1. Open the notebook in **Google Colab**
2. Select a **GPU** runtime
3. Run the cells sequentially

Some paths and settings are configured for Colab and may require small changes
for local execution.

## Data
The notebook uses example images and public or user-provided inputs for
demonstration purposes. No proprietary or restricted datasets are included
in this repository.

## Author
Marte Johanne Puck  
MSc Advanced Mechanical Engineering

## Notes
This notebook reflects exploratory MSc-level work.
Model outputs and parameters are intended for demonstration and qualitative
analysis rather than optimized benchmarking.
